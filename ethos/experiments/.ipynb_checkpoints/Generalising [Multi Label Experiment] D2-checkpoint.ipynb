{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IlqliX9gdv8D",
    "outputId": "f98cb9d3-dad6-4e42-b5fe-99adc6278c4a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#####################################################################\n",
    "#             Generalising on Multi Label Level                     #\n",
    "#####################################################################\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from utilities.preprocess import Preproccesor\n",
    "from utilities.attention_layer import Attention\n",
    "from utilities.helping_functions import create_embedding_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Dense, \\\n",
    "    LSTM, Conv1D, Dropout, concatenate\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, make_scorer, fbeta_score, multilabel_confusion_matrix,\\\n",
    "                            average_precision_score, precision_score, recall_score\n",
    "\n",
    "def average_precision_wrapper(y, y_pred, view):\n",
    "    return average_precision_score(y, y_pred.toarray(),average=view)\n",
    "\n",
    "hamm_scorer = make_scorer(hamming_loss, greater_is_better=False)\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ETHOS and the dataset D2: Ousidhoum, Nedjma, et al. \"Multilingual and multi-aspect hate speech analysis.\" arXiv preprint arXiv:1908.11049 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SM-R5Y-7f10f"
   },
   "source": [
    "We will load our data without preprocessing them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets contained: {'origin', 'religion', 'disability', 'sexual_orientation', 'other', 'gender'}\n"
     ]
    }
   ],
   "source": [
    "X, yt, y = Preproccesor.load_multi_label_data(True,False) #yt has continuous data, y has binary\n",
    "V, v_s, v_d, v_a, v_t, v_g, v_n = Preproccesor.load_mlma(True,False)\n",
    "label_names = [\"violence\",\"directed_vs_generalized\",\"gender\",\"race\",\"national_origin\",\"disability\",\"religion\",\"sexual_orientation\"]\n",
    "\n",
    "print(\"Targets contained:\",set(v_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hostility:',\n",
       " {'abusive',\n",
       "  'abusive_disrespectful',\n",
       "  'abusive_disrespectful_hateful_normal',\n",
       "  'abusive_hateful',\n",
       "  'abusive_normal',\n",
       "  'abusive_offensive',\n",
       "  'abusive_offensive_disrespectful_hateful_normal',\n",
       "  'abusive_offensive_disrespectful_normal',\n",
       "  'abusive_offensive_hateful_disrespectful',\n",
       "  'abusive_offensive_hateful_disrespectful_normal',\n",
       "  'abusive_offensive_hateful_normal',\n",
       "  'abusive_offensive_normal',\n",
       "  'disrespectful',\n",
       "  'disrespectful_hateful',\n",
       "  'disrespectful_normal',\n",
       "  'fearful',\n",
       "  'fearful_abusive',\n",
       "  'fearful_abusive_disrespectful_hateful_normal',\n",
       "  'fearful_abusive_disrespectful_normal',\n",
       "  'fearful_abusive_hateful_disrespectful',\n",
       "  'fearful_abusive_hateful_disrespectful_normal',\n",
       "  'fearful_abusive_hateful_normal',\n",
       "  'fearful_abusive_offensive_disrespectful',\n",
       "  'fearful_abusive_offensive_disrespectful_normal',\n",
       "  'fearful_abusive_offensive_hateful',\n",
       "  'fearful_abusive_offensive_hateful_disrespectful',\n",
       "  'fearful_abusive_offensive_hateful_normal',\n",
       "  'fearful_abusive_offensive_normal',\n",
       "  'fearful_disrespectful',\n",
       "  'fearful_hateful',\n",
       "  'fearful_hateful_disrespectful_normal',\n",
       "  'fearful_normal',\n",
       "  'fearful_offensive',\n",
       "  'fearful_offensive_disrespectful_hateful_normal',\n",
       "  'fearful_offensive_disrespectful_normal',\n",
       "  'fearful_offensive_hateful_disrespectful',\n",
       "  'fearful_offensive_hateful_disrespectful_normal',\n",
       "  'fearful_offensive_hateful_normal',\n",
       "  'hateful',\n",
       "  'hateful_disrespectful',\n",
       "  'hateful_normal',\n",
       "  'normal',\n",
       "  'offensive',\n",
       "  'offensive_disrespectful',\n",
       "  'offensive_hateful',\n",
       "  'offensive_hateful_disrespectful',\n",
       "  'offensive_hateful_disrespectful_normal',\n",
       "  'offensive_normal'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hostility:\",set(v_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our labels for the D2, based on the following rules. Of course sadly, it possible contains errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_v = []\n",
    "for i in range(len(V)):\n",
    "    temp_y = []\n",
    "    for j in range(len(label_names)):\n",
    "        if j == 0:\n",
    "            if 'abusive' in v_s[i] or 'hateful' in v_s[i] or 'fearful' in v_s[i]:\n",
    "                temp_y.append(1)\n",
    "            else:\n",
    "                temp_y.append(0)\n",
    "        elif j == 1:\n",
    "            if 'indirect' in v_d[i]:\n",
    "                temp_y.append(0)\n",
    "            else:\n",
    "                temp_y.append(1)\n",
    "    temp_c = [0]*6\n",
    "    if 'gender' in v_t[i]:\n",
    "        temp_c[0] = 1\n",
    "    if 'race' in v_t[i] or 'race' in v_g[i] or 'asians' in v_g[i] or 'african_descent' in v_g[i]:\n",
    "        temp_c[1] = 1\n",
    "    if 'origin' in v_t[i]:\n",
    "        temp_c[2] = 1\n",
    "    if 'disability' in v_t[i]:\n",
    "        temp_c[3] = 1\n",
    "    if 'religion' in v_t[i]:\n",
    "        temp_c[4] = 1\n",
    "    if 'sexual_orientation' in v_t[i]:\n",
    "        temp_c[5] = 1\n",
    "    for k in temp_c:\n",
    "        temp_y.append(k)\n",
    "    y_v.append(temp_y)\n",
    "y_valid = np.array(y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15001, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 15000\n",
    "max_len = 150\n",
    "emb_ma = 1\n",
    "embed_size = 150\n",
    "\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features, oov_token = True)\n",
    "tk.fit_on_texts(np.concatenate((X,V)))\n",
    "tokenized = tk.texts_to_sequences(X)\n",
    "x_train = pad_sequences(tokenized, maxlen=max_len)\n",
    "tokenized = tk.texts_to_sequences(V)\n",
    "x_valid = pad_sequences(tokenized, maxlen=max_len)\n",
    "embedding_matrix = create_embedding_matrix(emb_ma, tk, max_features)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = \"final.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=50)\n",
    "main_input1 = Input(shape=(max_len,), name='main_input1')\n",
    "x1 = (Embedding(max_features + 1, 300, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input1)\n",
    "x1 = SpatialDropout1D(0.4)(x1)\n",
    "x2 = Bidirectional(LSTM(75, dropout=0.5, return_sequences=True))(x1)\n",
    "x = Dropout(0.55)(x2)\n",
    "x = Bidirectional(LSTM(50, dropout=0.5, return_sequences=True))(x)\n",
    "hidden = concatenate([\n",
    "    Attention(max_len)(x1),\n",
    "    Attention(max_len)(x2),\n",
    "    Attention(max_len)(x)\n",
    "])\n",
    "hidden = Dense(32, activation='selu')(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "hidden = Dense(16, activation='selu')(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "output_lay1 = Dense(8, activation='sigmoid')(hidden)\n",
    "model = Model(inputs=[main_input1], outputs=output_lay1)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, y, validation_data=(x_valid, y_valid),\n",
    "#          batch_size=64, epochs=150, verbose=1, shuffle=True, callbacks=[check_point,early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is trained already, we can just load our weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filepath='weights/ethos_multi_label.hdf5')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we predict the instances of D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pr = model.predict(x_train)\n",
    "vpp = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_results = {}\n",
    "y_results[\"Data\"] = []\n",
    "y_results[\"F1 Samples\"] = []\n",
    "y_results[\"F1 Macro\"] = []\n",
    "y_results[\"F1 Micro\"] = []\n",
    "y_results[\"Pr Samples\"] = []\n",
    "y_results[\"Pr Macro\"] = []\n",
    "y_results[\"Pr Micro\"] = []\n",
    "y_results[\"Re Samples\"] = []\n",
    "y_results[\"Re Macro\"] = []\n",
    "y_results[\"Re Micro\"] = []\n",
    "y_results[\"Accuracy\"] = []\n",
    "y_results[\"Hamming\"] = []\n",
    "def results(y_t, y_p, y_results, name):\n",
    "    thr = 0.367\n",
    "    y_pr = []\n",
    "    for i in y_p:\n",
    "        temp_y = []\n",
    "        cou = 0\n",
    "        for j in i:\n",
    "            if j > thr:\n",
    "                temp_y.append(1)\n",
    "            else:\n",
    "                temp_y.append(0)\n",
    "            cou = cou + 1\n",
    "        y_pr.append(temp_y)\n",
    "    y_p = np.array(y_pr)\n",
    "    y_results[\"Data\"].append(name)\n",
    "    y_results[\"F1 Samples\"].append(f1_score(y_t, y_p, average='samples'))\n",
    "    y_results[\"F1 Macro\"].append(f1_score(y_t, y_p, average='macro'))\n",
    "    y_results[\"F1 Micro\"].append(f1_score(y_t, y_p, average='micro'))\n",
    "    y_results[\"Pr Samples\"].append(precision_score(y_t, y_p, average='samples'))\n",
    "    y_results[\"Pr Macro\"].append(precision_score(y_t, y_p, average='macro'))\n",
    "    y_results[\"Pr Micro\"].append(precision_score(y_t, y_p, average='micro'))\n",
    "    y_results[\"Re Samples\"].append(recall_score(y_t, y_p, average='samples'))\n",
    "    y_results[\"Re Macro\"].append(recall_score(y_t, y_p, average='macro'))\n",
    "    y_results[\"Re Micro\"].append(recall_score(y_t, y_p, average='micro'))\n",
    "    y_results[\"Accuracy\"].append(accuracy_score(y_t, y_p))\n",
    "    y_results[\"Hamming\"].append(hamming_loss(y_t, y_p))\n",
    "    return y_results\n",
    "y_results = results(y, y_pr, y_results, 'Train')\n",
    "y_results = results(y_valid, vpp, y_results, 'Valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data      \tTrain\tValid\n",
      "F1 Samples\t0.891\t0.385\n",
      "F1 Macro\t0.92\t0.406\n",
      "F1 Micro\t0.89\t0.416\n",
      "Pr Samples\t0.931\t0.401\n",
      "Pr Macro\t0.931\t0.405\n",
      "Pr Micro\t0.905\t0.378\n",
      "Re Samples\t0.894\t0.426\n",
      "Re Macro\t0.909\t0.517\n",
      "Re Micro\t0.875\t0.461\n",
      "Accuracy\t0.67\t0.175\n",
      "Hamming \t0.045\t0.206\n"
     ]
    }
   ],
   "source": [
    "for k,v in y_results.items():\n",
    "    if k == 'Data':\n",
    "        print(\"{}\\t{}\\t{}\".format(k+'      ',v[0],v[1]))\n",
    "    elif k == 'Hamming':\n",
    "        print(\"{}\\t{}\\t{}\".format(k+' ',round(v[0],3),round(v[1],3)))\n",
    "    else:\n",
    "        print(\"{}\\t{}\\t{}\".format(k,round(v[0],3),round(v[1],3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance: if america had another years of obama ideology via hillary we would be well on our way to being shithole country\n",
      "  Sentiment: fearful_abusive_hateful_disrespectful_normal\n",
      "    Incites Violence: 0.12612055\n",
      "    No Violence: 0.8738794475793839\n",
      "  Scope: indirect\n",
      "    Direct: 0.047181424\n",
      "    Generalized: 0.9528185762465\n",
      "  Hate Group: origin other\n",
      "    Gender: 0.0036667627\n",
      "    Race: 0.014548816\n",
      "    Origin: 0.84192735\n",
      "    Disability: 0.01104671\n",
      "    Religion: 0.03923865\n",
      "    Sexual Orientation: 0.0065618805\n",
      "\n",
      "Instance: most canadians have never met seen or associated with person who are currently labelled as retarded plan u2026 url\n",
      "  Sentiment: offensive\n",
      "    Incites Violence: 0.15779072\n",
      "    No Violence: 0.8422092795372009\n",
      "  Scope: indirect\n",
      "    Direct: 0.11329159\n",
      "    Generalized: 0.8867084085941315\n",
      "  Hate Group: disability special_needs\n",
      "    Gender: 0.0217287\n",
      "    Race: 0.026790421\n",
      "    Origin: 0.71264493\n",
      "    Disability: 0.16527565\n",
      "    Religion: 0.019782986\n",
      "    Sexual Orientation: 0.01995425\n",
      "\n",
      "Instance: hahaha grow up faggot url\n",
      "  Sentiment: offensive\n",
      "    Incites Violence: 0.31715903\n",
      "    No Violence: 0.6828409731388092\n",
      "  Scope: indirect\n",
      "    Direct: 0.46509778\n",
      "    Generalized: 0.5349022150039673\n",
      "  Hate Group: sexual_orientation women\n",
      "    Gender: 0.07112672\n",
      "    Race: 0.014502591\n",
      "    Origin: 0.052016307\n",
      "    Disability: 0.054761425\n",
      "    Religion: 0.029569013\n",
      "    Sexual Orientation: 0.84141755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Instance:', V[i])\n",
    "    print('  Sentiment:',v_s[i])\n",
    "    print('    Incites Violence:',vpp[i][0])\n",
    "    print('    No Violence:',1-vpp[i][0])\n",
    "    print('  Scope:',v_d[i])\n",
    "    print('    Direct:',vpp[i][1])\n",
    "    print('    Generalized:',1-vpp[i][1])\n",
    "    print('  Hate Group:',v_t[i],v_g[i])\n",
    "    print('    Gender:',vpp[i][2])\n",
    "    print('    Race:',vpp[i][3])\n",
    "    print('    Origin:',vpp[i][4])\n",
    "    print('    Disability:',vpp[i][5])\n",
    "    print('    Religion:',vpp[i][6])\n",
    "    print('    Sexual Orientation:',vpp[i][7])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results per label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violence\n",
      " MLMA Accuracy 0.5086\n",
      " MLMA F1 0.5948\n",
      " Train F1 NH: 0.725 H: 0.2909\n",
      "directed_vs_generalized\n",
      " MLMA Accuracy 0.5528\n",
      " MLMA F1 0.5539\n",
      " Train F1 NH: 0.5936 H: 0.1998\n",
      "gender\n",
      " MLMA Accuracy 0.7034\n",
      " MLMA F1 0.8771\n",
      " Train F1 NH: 0.9294 H: 0.4659\n",
      "race\n",
      " MLMA Accuracy 0.7597\n",
      " MLMA F1 0.9278\n",
      " Train F1 NH: 0.9461 H: 0.2406\n",
      "national_origin\n",
      " MLMA Accuracy 0.6788\n",
      " MLMA F1 0.6897\n",
      " Train F1 NH: 0.7489 H: 0.6123\n",
      "disability\n",
      " MLMA Accuracy 0.6964\n",
      " MLMA F1 0.838\n",
      " Train F1 NH: 0.9106 H: 0.5344\n",
      "religion\n",
      " MLMA Accuracy 0.7165\n",
      " MLMA F1 0.9765\n",
      " Train F1 NH: 0.9851 H: 0.2731\n",
      "sexual_orientation\n",
      " MLMA Accuracy 0.8983\n",
      " MLMA F1 0.9421\n",
      " Train F1 NH: 0.965 H: 0.7129\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(label_names)):\n",
    "    print(label_names[l])\n",
    "    y_t = y_valid[:,l:l+1]\n",
    "    y_t = np.array([k[0] for k in y_t])\n",
    "    y_p = vpp[:,l:l+1]          \n",
    "    y_p = np.array([1 if k[0] > 0.34375 else 0 for k in y_p])\n",
    "    print(' MLMA Accuracy',round(balanced_accuracy_score(y_t, y_p),4))\n",
    "    print(' MLMA F1',round(f1_score(y_t, y_p, average='weighted'),4))\n",
    "    print(' Train F1 NH:',round(f1_score(y_t, y_p, average=None)[0],4),'H:',round(f1_score(y_t, y_p, average=None)[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations on low performing 'race' label:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ethos did not classify as race, but they contained hate speech towards race (according D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96  well my parents like carikla ching chong guy in your college\n",
      "166 yay kelas ching chong today\n",
      "254  okay ching chong\n",
      "335 remember it was some ching chong hoe on here that was flexin on him years ago found out they was fuckin smh\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    if y_valid[i][3]==1 and vpp[i][3] < 0.36:\n",
    "        print(i,V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date ethos predicted as hate speech content towards race. However, D2 did not had the race label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44  so many dead and illegal aliens will vote blue in november dems aren worried\n",
      "56 literally waited at red light for five minutes am such fucking retard\n",
      "89 just found out that everyones origins are from africa am aloud to say nigger nohatejustasking\n",
      "121 lol you know what mean nigger got caught again url\n",
      "124  nigga it started lagging so we left retard\n",
      "126 remember negro my open mind got skin door wayne\n",
      "128   love you negro\n",
      "132 keep in mind negro so my open mind got screen door impressive carterv\n",
      "159 when white says wassup nigger because you made video calling yourself nigger url\n",
      "175 it is okay to be white auspol url\n",
      "192 black youtubers will you be demonetized if you say nigger really wanna say nigger on my channel\n",
      "198  this shooting is totally leftist setup and an attack on our second amendment rights find the perps u2026 url\n",
      "200 it okay to be white motion suspiciously delayed url\n",
      "212 my baby told me was beautiful with my bare face and nigger twist url\n",
      "216 in pedo speak hotdogs means little boys nso muslim faggot  imported bunch of black kids from chi u2026 url\n",
      "224 white gringxs be like u201chow is you saying u201cthe office u201d is bad any better than me calling you spic u201d\n",
      "225  it is decided by race so indians carry caucasoid australoid and mongoloid races and so do the intermedi u2026 url\n",
      "230  yeah non racists reflexively shout racist things late and the spic barista is too slow shout u201chey be u2026 url\n",
      "236  die nigger\n",
      "245  do it nigger\n",
      "267 this is accurate thank nigger twink url\n",
      "276  see the type of nigger you are hmph\n",
      "283 it okay to be white\n",
      "288  nigger jew\n",
      "303  prayers for that  nigger at this time\n",
      "312  that what you get for eating pickles you nigger\n",
      "339  fucking nigger try to kill me bet you dont do it cause youre american hahaha white nigger com url\n",
      "351 people always say black people are from africa but am from the falkland islands so fuck off\n",
      "354    now yes this politically motivated terrorist is white and leftist\n",
      "355 fuck you will listen to africa by toto if want\n",
      "362 shit that should have realized earlier the phrase it is okay to be white is the fisher price version of the 14 words\n",
      "380 deep slave nigger sigh\n",
      "410  bring me nigger\n",
      "424 my nigger lion heart wish you god forward ever back ward never man me say\n",
      "425 this is nigger fuck\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cou = 0\n",
    "for i in range(len(y)):\n",
    "    if y_valid[i][3]==0 and vpp[i][3] > 0.36:\n",
    "        print(i,V[i])\n",
    "        cou = cou + 1\n",
    "cou"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN+Class.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
